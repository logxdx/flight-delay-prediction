{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":211961,"sourceType":"datasetVersion","datasetId":91397}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/logxdx158/flight-delay-prediction?scriptVersionId=207524785\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T11:20:29.772979Z","iopub.execute_input":"2024-11-15T11:20:29.775593Z","iopub.status.idle":"2024-11-15T11:20:29.797944Z","shell.execute_reply.started":"2024-11-15T11:20:29.775497Z","shell.execute_reply":"2024-11-15T11:20:29.796343Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/mini-flight-delay-prediction/flight_delays_test.csv\n/kaggle/input/mini-flight-delay-prediction/flight_delays_train.csv\n","output_type":"stream"}],"execution_count":99},{"cell_type":"code","source":"import pandas as pd\n\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T11:20:29.800452Z","iopub.execute_input":"2024-11-15T11:20:29.801657Z","iopub.status.idle":"2024-11-15T11:20:29.807875Z","shell.execute_reply.started":"2024-11-15T11:20:29.801587Z","shell.execute_reply":"2024-11-15T11:20:29.806669Z"}},"outputs":[],"execution_count":100},{"cell_type":"code","source":"\n#  load dataset\n\ndf_train = pd.read_csv(\"/kaggle/input/mini-flight-delay-prediction/flight_delays_train.csv\")\n\ndf_test = pd.read_csv(\"/kaggle/input/mini-flight-delay-prediction/flight_delays_test.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T11:20:29.809409Z","iopub.execute_input":"2024-11-15T11:20:29.809839Z","iopub.status.idle":"2024-11-15T11:20:30.144709Z","shell.execute_reply.started":"2024-11-15T11:20:29.8098Z","shell.execute_reply":"2024-11-15T11:20:30.143313Z"}},"outputs":[],"execution_count":101},{"cell_type":"code","source":"df_train.info()\n\nprint()\n\ndf_test.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T11:20:30.14782Z","iopub.execute_input":"2024-11-15T11:20:30.148225Z","iopub.status.idle":"2024-11-15T11:20:30.315461Z","shell.execute_reply.started":"2024-11-15T11:20:30.148185Z","shell.execute_reply":"2024-11-15T11:20:30.313953Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 100000 entries, 0 to 99999\nData columns (total 9 columns):\n #   Column             Non-Null Count   Dtype \n---  ------             --------------   ----- \n 0   Month              100000 non-null  object\n 1   DayofMonth         100000 non-null  object\n 2   DayOfWeek          100000 non-null  object\n 3   DepTime            100000 non-null  int64 \n 4   UniqueCarrier      100000 non-null  object\n 5   Origin             100000 non-null  object\n 6   Dest               100000 non-null  object\n 7   Distance           100000 non-null  int64 \n 8   dep_delayed_15min  100000 non-null  object\ndtypes: int64(2), object(7)\nmemory usage: 6.9+ MB\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 100000 entries, 0 to 99999\nData columns (total 8 columns):\n #   Column         Non-Null Count   Dtype \n---  ------         --------------   ----- \n 0   Month          100000 non-null  object\n 1   DayofMonth     100000 non-null  object\n 2   DayOfWeek      100000 non-null  object\n 3   DepTime        100000 non-null  int64 \n 4   UniqueCarrier  100000 non-null  object\n 5   Origin         100000 non-null  object\n 6   Dest           100000 non-null  object\n 7   Distance       100000 non-null  int64 \ndtypes: int64(2), object(6)\nmemory usage: 6.1+ MB\n","output_type":"stream"}],"execution_count":102},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"print(\"train:\")\n\nprint(df_train.head())\n\nprint(\"\\n\\ntest:\")\n\nprint(df_test.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T11:20:30.317918Z","iopub.execute_input":"2024-11-15T11:20:30.318426Z","iopub.status.idle":"2024-11-15T11:20:30.33529Z","shell.execute_reply.started":"2024-11-15T11:20:30.31837Z","shell.execute_reply":"2024-11-15T11:20:30.33372Z"}},"outputs":[{"name":"stdout","text":"train:\n  Month DayofMonth DayOfWeek  DepTime UniqueCarrier Origin Dest  Distance  \\\n0   c-8       c-21       c-7     1934            AA    ATL  DFW       732   \n1   c-4       c-20       c-3     1548            US    PIT  MCO       834   \n2   c-9        c-2       c-5     1422            XE    RDU  CLE       416   \n3  c-11       c-25       c-6     1015            OO    DEN  MEM       872   \n4  c-10        c-7       c-6     1828            WN    MDW  OMA       423   \n\n  dep_delayed_15min  \n0                 N  \n1                 N  \n2                 N  \n3                 N  \n4                 Y  \n\n\ntest:\n  Month DayofMonth DayOfWeek  DepTime UniqueCarrier Origin Dest  Distance\n0   c-7       c-25       c-3      615            YV    MRY  PHX       598\n1   c-4       c-17       c-2      739            WN    LAS  HOU      1235\n2  c-12        c-2       c-7      651            MQ    GSP  ORD       577\n3   c-3       c-25       c-7     1614            WN    BWI  MHT       377\n4   c-6        c-6       c-3     1505            UA    ORD  STL       258\n","output_type":"stream"}],"execution_count":103},{"cell_type":"code","source":"print(\"Unique values of columns\\n----------------------------\")\n\nfor i in df_train.columns:\n\n    print(i,\":\", len(df_train[i].unique()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T11:20:30.337194Z","iopub.execute_input":"2024-11-15T11:20:30.33824Z","iopub.status.idle":"2024-11-15T11:20:30.406771Z","shell.execute_reply.started":"2024-11-15T11:20:30.338184Z","shell.execute_reply":"2024-11-15T11:20:30.405367Z"}},"outputs":[{"name":"stdout","text":"Unique values of columns\n----------------------------\nMonth : 12\nDayofMonth : 31\nDayOfWeek : 7\nDepTime : 1300\nUniqueCarrier : 22\nOrigin : 289\nDest : 289\nDistance : 1310\ndep_delayed_15min : 2\n","output_type":"stream"}],"execution_count":104},{"cell_type":"code","source":"encoding_labels = ['UniqueCarrier', 'Origin', 'Dest']\n\nfor i in encoding_labels:\n    values, counts = np.unique(df_train[i], return_counts=True)\n    print(f\"\\n\\n{i}:\\n---------------------\")\n    print(\"Data present in Train but not in test:\")\n    for j, c in zip(values, counts):\n        if j not in  df_test[i].unique():\n            print(f\"{j}: {c}\")\n\n    values, counts = np.unique(df_test[i], return_counts=True)\n    print(\"\\nData present in test but not in train:\")\n    for j, c in zip(values, counts):\n        if j not in  df_train[i].unique():\n            print(f\"{j}: {c}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T11:20:30.408585Z","iopub.execute_input":"2024-11-15T11:20:30.40904Z","iopub.status.idle":"2024-11-15T11:20:40.982379Z","shell.execute_reply.started":"2024-11-15T11:20:30.408996Z","shell.execute_reply":"2024-11-15T11:20:40.980962Z"}},"outputs":[{"name":"stdout","text":"\n\nUniqueCarrier:\n---------------------\nData present in Train but not in test:\nDH: 966\nHP: 1378\nTZ: 446\n\nData present in test but not in train:\n9E: 3461\n\n\nOrigin:\n---------------------\nData present in Train but not in test:\nCDC: 10\nHKY: 7\nHVN: 7\nISO: 8\nPIE: 3\nVCT: 1\nVIS: 1\n\nData present in test but not in train:\nALO: 2\nBJI: 3\nCMX: 2\nEAU: 1\nELM: 13\nEWN: 7\nHHH: 13\nINL: 3\nMTH: 3\nPLN: 5\nPMD: 4\nRHI: 3\nROW: 3\nSLE: 12\nSOP: 2\nSUX: 4\nTTN: 8\nYKM: 11\n\n\nDest:\n---------------------\nData present in Train but not in test:\nCDC: 7\nGST: 2\nHKY: 5\nHVN: 10\nISO: 9\nPIE: 5\nTEX: 1\nVCT: 1\nVIS: 7\n\nData present in test but not in train:\nADK: 1\nALO: 3\nBJI: 1\nCMX: 2\nEAU: 1\nELM: 14\nEWN: 7\nHHH: 23\nINL: 3\nMTH: 1\nOAJ: 14\nPLN: 5\nPMD: 3\nRHI: 4\nROW: 2\nSLE: 3\nSUX: 5\nYKM: 6\n","output_type":"stream"}],"execution_count":105},{"cell_type":"code","source":"for i in encoding_labels:\n    print(i)\n    # For values in train but not in test\n    df_train[i] = df_train[i].where(df_train[i].isin(df_test[i].unique()), 'Unknown')\n    \n    # For values in test but not in train\n    df_test[i] = df_test[i].where(df_test[i].isin(df_train[i].unique()), 'Unknown')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T11:20:40.984646Z","iopub.execute_input":"2024-11-15T11:20:40.985183Z","iopub.status.idle":"2024-11-15T11:20:41.115597Z","shell.execute_reply.started":"2024-11-15T11:20:40.985126Z","shell.execute_reply":"2024-11-15T11:20:41.113933Z"}},"outputs":[{"name":"stdout","text":"UniqueCarrier\nOrigin\nDest\n","output_type":"stream"}],"execution_count":106},{"cell_type":"code","source":"# Function to remove 'c-' prefix from relevant columns\ndef clean_column_values(df, columns):\n    for col in columns:\n        df[col] = df[col].str.replace('c-', '', regex=False).astype(int)\n    return df\n\n# Clean the 'Month', 'DayofMonth', 'DayOfWeek' columns in the train and test datasets\ncolumns_to_clean = ['Month', 'DayofMonth', 'DayOfWeek']\ndf_train = clean_column_values(df_train, columns_to_clean)\ndf_test = clean_column_values(df_test, columns_to_clean)\n\n# Convert the target column ('dep_delayed_15min') to binary values (1 for 'Y', 0 for 'N')\ndf_train['dep_delayed_15min'] = df_train['dep_delayed_15min'].apply(lambda x: 1 if x == 'Y' else 0)\n\n# Additional feature engineering (extract hour and minute from DepTime)\ndf_train['DepHour'] = df_train['DepTime'] // 100\ndf_train['DepMinute'] = df_train['DepTime'] % 100\ndf_test['DepHour'] = df_test['DepTime'] // 100\ndf_test['DepMinute'] = df_test['DepTime'] % 100\n\n\nfrom sklearn.preprocessing import OneHotEncoder\n# One-hot encode categorical variables\ncategorical_columns = ['Month', 'DayOfWeek', 'UniqueCarrier', 'Origin', 'Dest']\nencoder = OneHotEncoder(drop='first', sparse_output=False)\nencoded_features_train = encoder.fit_transform(df_train[categorical_columns])\nencoded_features_test = encoder.transform(df_test[categorical_columns])\n\n# Create DataFrames for encoded features and join with original datasets\nencoded_df_train = pd.DataFrame(encoded_features_train, columns=encoder.get_feature_names_out(categorical_columns))\nencoded_df_test = pd.DataFrame(encoded_features_test, columns=encoder.get_feature_names_out(categorical_columns))\n\ndf_train = df_train.join(encoded_df_train)\ndf_test = df_test.join(encoded_df_test)\n\n# Drop original categorical columns\ndf_train.drop(columns=categorical_columns + ['DepTime'], inplace=True)\ndf_test.drop(columns=categorical_columns + ['DepTime'], inplace=True)\n\n# Optional: Distance binning\ndf_train['DistanceCategory'] = pd.cut(df_train['Distance'], bins=[0, 500, 1000, 1500, float('inf')], labels=[0, 1, 2, 3])\ndf_test['DistanceCategory'] = pd.cut(df_test['Distance'], bins=[0, 500, 1000, 1500, float('inf')], labels=[0, 1, 2, 3])\n\n# Split dataset for training\nX_train = df_train.drop(columns=['dep_delayed_15min'])\ny_train = df_train['dep_delayed_15min']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T11:20:41.117773Z","iopub.execute_input":"2024-11-15T11:20:41.118248Z","iopub.status.idle":"2024-11-15T11:20:48.513544Z","shell.execute_reply.started":"2024-11-15T11:20:41.118207Z","shell.execute_reply":"2024-11-15T11:20:48.512266Z"}},"outputs":[],"execution_count":107},{"cell_type":"code","source":"df_train.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T11:20:48.518485Z","iopub.execute_input":"2024-11-15T11:20:48.5197Z","iopub.status.idle":"2024-11-15T11:20:48.556516Z","shell.execute_reply.started":"2024-11-15T11:20:48.519657Z","shell.execute_reply":"2024-11-15T11:20:48.554585Z"}},"outputs":[{"execution_count":108,"output_type":"execute_result","data":{"text/plain":"   DayofMonth  Distance  dep_delayed_15min  DepHour  DepMinute  Month_2  \\\n0          21       732                  0       19         34      0.0   \n1          20       834                  0       15         48      0.0   \n2           2       416                  0       14         22      0.0   \n3          25       872                  0       10         15      0.0   \n4           7       423                  1       18         28      0.0   \n\n   Month_3  Month_4  Month_5  Month_6  ...  Dest_TYS  Dest_Unknown  Dest_VLD  \\\n0      0.0      0.0      0.0      0.0  ...       0.0           0.0       0.0   \n1      0.0      1.0      0.0      0.0  ...       0.0           0.0       0.0   \n2      0.0      0.0      0.0      0.0  ...       0.0           0.0       0.0   \n3      0.0      0.0      0.0      0.0  ...       0.0           0.0       0.0   \n4      0.0      0.0      0.0      0.0  ...       0.0           0.0       0.0   \n\n   Dest_VPS  Dest_WRG  Dest_WYS  Dest_XNA  Dest_YAK  Dest_YUM  \\\n0       0.0       0.0       0.0       0.0       0.0       0.0   \n1       0.0       0.0       0.0       0.0       0.0       0.0   \n2       0.0       0.0       0.0       0.0       0.0       0.0   \n3       0.0       0.0       0.0       0.0       0.0       0.0   \n4       0.0       0.0       0.0       0.0       0.0       0.0   \n\n   DistanceCategory  \n0                 1  \n1                 1  \n2                 0  \n3                 1  \n4                 0  \n\n[5 rows x 604 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>DayofMonth</th>\n      <th>Distance</th>\n      <th>dep_delayed_15min</th>\n      <th>DepHour</th>\n      <th>DepMinute</th>\n      <th>Month_2</th>\n      <th>Month_3</th>\n      <th>Month_4</th>\n      <th>Month_5</th>\n      <th>Month_6</th>\n      <th>...</th>\n      <th>Dest_TYS</th>\n      <th>Dest_Unknown</th>\n      <th>Dest_VLD</th>\n      <th>Dest_VPS</th>\n      <th>Dest_WRG</th>\n      <th>Dest_WYS</th>\n      <th>Dest_XNA</th>\n      <th>Dest_YAK</th>\n      <th>Dest_YUM</th>\n      <th>DistanceCategory</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>21</td>\n      <td>732</td>\n      <td>0</td>\n      <td>19</td>\n      <td>34</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20</td>\n      <td>834</td>\n      <td>0</td>\n      <td>15</td>\n      <td>48</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>416</td>\n      <td>0</td>\n      <td>14</td>\n      <td>22</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>25</td>\n      <td>872</td>\n      <td>0</td>\n      <td>10</td>\n      <td>15</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>423</td>\n      <td>1</td>\n      <td>18</td>\n      <td>28</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 604 columns</p>\n</div>"},"metadata":{}}],"execution_count":108},{"cell_type":"code","source":"df_test.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T11:20:48.558393Z","iopub.execute_input":"2024-11-15T11:20:48.558952Z","iopub.status.idle":"2024-11-15T11:20:48.594719Z","shell.execute_reply.started":"2024-11-15T11:20:48.558895Z","shell.execute_reply":"2024-11-15T11:20:48.593207Z"}},"outputs":[{"execution_count":109,"output_type":"execute_result","data":{"text/plain":"   DayofMonth  Distance  DepHour  DepMinute  Month_2  Month_3  Month_4  \\\n0          25       598        6         15      0.0      0.0      0.0   \n1          17      1235        7         39      0.0      0.0      1.0   \n2           2       577        6         51      0.0      0.0      0.0   \n3          25       377       16         14      0.0      1.0      0.0   \n4           6       258       15          5      0.0      0.0      0.0   \n\n   Month_5  Month_6  Month_7  ...  Dest_TYS  Dest_Unknown  Dest_VLD  Dest_VPS  \\\n0      0.0      0.0      1.0  ...       0.0           0.0       0.0       0.0   \n1      0.0      0.0      0.0  ...       0.0           0.0       0.0       0.0   \n2      0.0      0.0      0.0  ...       0.0           0.0       0.0       0.0   \n3      0.0      0.0      0.0  ...       0.0           0.0       0.0       0.0   \n4      0.0      1.0      0.0  ...       0.0           0.0       0.0       0.0   \n\n   Dest_WRG  Dest_WYS  Dest_XNA  Dest_YAK  Dest_YUM  DistanceCategory  \n0       0.0       0.0       0.0       0.0       0.0                 1  \n1       0.0       0.0       0.0       0.0       0.0                 2  \n2       0.0       0.0       0.0       0.0       0.0                 1  \n3       0.0       0.0       0.0       0.0       0.0                 0  \n4       0.0       0.0       0.0       0.0       0.0                 0  \n\n[5 rows x 603 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>DayofMonth</th>\n      <th>Distance</th>\n      <th>DepHour</th>\n      <th>DepMinute</th>\n      <th>Month_2</th>\n      <th>Month_3</th>\n      <th>Month_4</th>\n      <th>Month_5</th>\n      <th>Month_6</th>\n      <th>Month_7</th>\n      <th>...</th>\n      <th>Dest_TYS</th>\n      <th>Dest_Unknown</th>\n      <th>Dest_VLD</th>\n      <th>Dest_VPS</th>\n      <th>Dest_WRG</th>\n      <th>Dest_WYS</th>\n      <th>Dest_XNA</th>\n      <th>Dest_YAK</th>\n      <th>Dest_YUM</th>\n      <th>DistanceCategory</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>25</td>\n      <td>598</td>\n      <td>6</td>\n      <td>15</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>17</td>\n      <td>1235</td>\n      <td>7</td>\n      <td>39</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>577</td>\n      <td>6</td>\n      <td>51</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>25</td>\n      <td>377</td>\n      <td>16</td>\n      <td>14</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>6</td>\n      <td>258</td>\n      <td>15</td>\n      <td>5</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 603 columns</p>\n</div>"},"metadata":{}}],"execution_count":109},{"cell_type":"code","source":"df_train.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T11:20:48.59673Z","iopub.execute_input":"2024-11-15T11:20:48.597248Z","iopub.status.idle":"2024-11-15T11:20:48.643475Z","shell.execute_reply.started":"2024-11-15T11:20:48.597195Z","shell.execute_reply":"2024-11-15T11:20:48.64218Z"},"jupyter":{"source_hidden":true}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 100000 entries, 0 to 99999\nColumns: 604 entries, DayofMonth to DistanceCategory\ndtypes: category(1), float64(598), int64(5)\nmemory usage: 460.1 MB\n","output_type":"stream"}],"execution_count":110},{"cell_type":"code","source":"df_test.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T11:20:48.645413Z","iopub.execute_input":"2024-11-15T11:20:48.645941Z","iopub.status.idle":"2024-11-15T11:20:48.690836Z","shell.execute_reply.started":"2024-11-15T11:20:48.645885Z","shell.execute_reply":"2024-11-15T11:20:48.689278Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 100000 entries, 0 to 99999\nColumns: 603 entries, DayofMonth to DistanceCategory\ndtypes: category(1), float64(598), int64(4)\nmemory usage: 459.4 MB\n","output_type":"stream"}],"execution_count":111},{"cell_type":"code","source":"y_train.value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T11:20:48.694914Z","iopub.execute_input":"2024-11-15T11:20:48.695407Z","iopub.status.idle":"2024-11-15T11:20:48.708116Z","shell.execute_reply.started":"2024-11-15T11:20:48.695359Z","shell.execute_reply":"2024-11-15T11:20:48.706687Z"}},"outputs":[{"execution_count":112,"output_type":"execute_result","data":{"text/plain":"dep_delayed_15min\n0    80956\n1    19044\nName: count, dtype: int64"},"metadata":{}}],"execution_count":112},{"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nsmote = SMOTE(random_state=42)\nX_train, y_train = df_train.drop('dep_delayed_15min', axis=1), df_train['dep_delayed_15min']\nX_resampled, y_resampled = smote.fit_resample(X_train, y_train)\ny_resampled.value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T11:20:48.709937Z","iopub.execute_input":"2024-11-15T11:20:48.710383Z","iopub.status.idle":"2024-11-15T11:21:02.517277Z","shell.execute_reply.started":"2024-11-15T11:20:48.71034Z","shell.execute_reply":"2024-11-15T11:21:02.515973Z"}},"outputs":[{"execution_count":113,"output_type":"execute_result","data":{"text/plain":"dep_delayed_15min\n0    80956\n1    80956\nName: count, dtype: int64"},"metadata":{}}],"execution_count":113},{"cell_type":"code","source":"import os\nimport shutil\n\nworking_dir = \"/kaggle/working/\"\n\n# Remove all files in the working directory\nfor filename in os.listdir(working_dir):\n    file_path = os.path.join(working_dir, filename)\n    try:\n        if os.path.isfile(file_path) or os.path.islink(file_path):\n            os.unlink(file_path)  # Remove file or link\n        elif os.path.isdir(file_path):\n            shutil.rmtree(file_path)  # Remove directory and its contents\n    except Exception as e:\n        print(f\"Failed to delete {file_path}. Reason: {e}\")\n\nprint(\"Kaggle working directory cleared.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T11:21:02.519119Z","iopub.execute_input":"2024-11-15T11:21:02.519541Z","iopub.status.idle":"2024-11-15T11:21:02.529995Z","shell.execute_reply.started":"2024-11-15T11:21:02.519501Z","shell.execute_reply":"2024-11-15T11:21:02.528405Z"}},"outputs":[{"name":"stdout","text":"Kaggle working directory cleared.\n","output_type":"stream"}],"execution_count":114},{"cell_type":"markdown","source":"# Training & Validation","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, stratify=y_train, test_size=0.2, random_state=42)\nX_resampled, X_val_resampled, y_resampled, y_val_resampled = train_test_split(X_resampled, y_resampled, stratify=y_resampled, test_size=0.2, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T11:21:02.531821Z","iopub.execute_input":"2024-11-15T11:21:02.532284Z","iopub.status.idle":"2024-11-15T11:21:05.264095Z","shell.execute_reply.started":"2024-11-15T11:21:02.53223Z","shell.execute_reply":"2024-11-15T11:21:05.262555Z"}},"outputs":[],"execution_count":115},{"cell_type":"code","source":"import pickle\nfrom sklearn.metrics import accuracy_score, f1_score\nimport os\n\n# Load each model and evaluate on the test data\ndef load_and_evaluate(model_file, X_test, y_test, fname, dataset):\n    # Load the model from the pickle file\n    with open(model_file, \"rb\") as file:\n        model = pickle.load(file)\n    \n    # Make predictions on the test data\n    y_pred = model.predict(X_test)\n    \n    # Calculate evaluation metrics\n    accuracy = accuracy_score(y_test, y_pred)\n    f1 = f1_score(y_test, y_pred)\n\n    with open(f\"{working_dir}{fname}.txt\", \"a\") as file:\n        file.write(f\"Model: {model_file}\\n\")\n        file.write(f\"Dataset: {dataset}\\n\")\n        file.write(f\"Accuracy: {accuracy:.4f}\\n\")\n        file.write(f\"F1 Score: {f1:.4f}\\n\")\n        file.write(\"-\" * 30 + \"\\n\")\n    \n    # Print the results\n    print(f\"Model: {model_file}\")\n    print(f\"Dataset: {dataset}\\n\")\n    print(f\"Accuracy: {accuracy:.4f}\")\n    print(f\"F1 Score: {f1:.4f}\")\n    print(\"-\" * 30)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T11:21:05.266157Z","iopub.execute_input":"2024-11-15T11:21:05.266567Z","iopub.status.idle":"2024-11-15T11:21:05.27772Z","shell.execute_reply.started":"2024-11-15T11:21:05.266525Z","shell.execute_reply":"2024-11-15T11:21:05.276209Z"}},"outputs":[],"execution_count":116},{"cell_type":"code","source":"import pickle\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, StackingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom xgboost import XGBClassifier\nimport multiprocessing as mp\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nmodels = {\n    \"DecisionTree\": DecisionTreeClassifier(max_depth=15, min_samples_split=5),\n    \"RandomForest\": RandomForestClassifier(max_depth=23, n_estimators=100),\n    \"AdaBoost\": AdaBoostClassifier(n_estimators=500),\n    \"GradientBoosting\": GradientBoostingClassifier(n_estimators=100, max_depth=3),\n    \n    # XGBoost with GPU support\n    \"XGBoost\": XGBClassifier(n_estimators=100, max_depth=3, use_label_encoder=False, eval_metric='logloss'),\n    \n    \"GaussianNB\": GaussianNB(),\n    \"LDA\": LinearDiscriminantAnalysis(),\n    \"QDA\": QuadraticDiscriminantAnalysis(),\n    \"LogisticRegression\": LogisticRegression(max_iter=1000),\n    \"KNN\": KNeighborsClassifier(n_neighbors=5),    \n    \"ExtraTrees\": ExtraTreesClassifier(n_estimators=100, max_depth=10),\n}\n\n# For Stacking Classifier, we’ll use a subset of the trained models as estimators\nstacking_estimators = [\n    ('RandomForest', RandomForestClassifier(max_depth=23, n_estimators=50)),\n    ('GradientBoosting', GradientBoostingClassifier(n_estimators=50, max_depth=3)),\n    ('AdaBoost', AdaBoostClassifier(n_estimators=50)),\n    ('LogisticRegression', LogisticRegression(max_iter=1000))\n]\nmodels[\"Stacking\"] = StackingClassifier(estimators=stacking_estimators, final_estimator=LogisticRegression())\n\n# Function to train and save each model\ndef train_and_save(model_name, model, X, y, suffix):\n    model.fit(X, y)\n    with open(f\"{working_dir}{model_name}_{suffix}.pkl\", \"wb\") as file:\n        pickle.dump(model, file)\n    print(f\"{model_name} trained on {suffix} data and saved.\")\n\n# Prepare data for multiprocessing\ntasks = []\nfor model_name, model in models.items():\n    # Add tasks for both original and resampled data\n    tasks.append((model_name, model, X_train, y_train, \"Original\"))\n    tasks.append((model_name, model, X_resampled, y_resampled, \"Resampled\"))\n\n# Use multiprocessing to run tasks in parallel\nif __name__ == \"__main__\":\n    with mp.Pool(processes=min(mp.cpu_count(), 8)) as pool:\n        pool.starmap(train_and_save, tasks)\n\n    print(\"All models have been trained and saved on both original and resampled data.\")\n","metadata":{"_kg_hide-output":true,"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T11:21:05.279968Z","iopub.execute_input":"2024-11-15T11:21:05.280481Z","iopub.status.idle":"2024-11-15T11:36:06.895251Z","shell.execute_reply.started":"2024-11-15T11:21:05.280437Z","shell.execute_reply":"2024-11-15T11:36:06.893475Z"}},"outputs":[{"name":"stdout","text":"DecisionTree trained on Original data and saved.\nGaussianNB trained on Original data and saved.\nRandomForest trained on Original data and saved.\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:926: UserWarning: Variables are collinear\n  warnings.warn(\"Variables are collinear\")\n","output_type":"stream"},{"name":"stdout","text":"QDA trained on Original data and saved.\nLDA trained on Original data and saved.\nLogisticRegression trained on Original data and saved.\nKNN trained on Original data and saved.\nExtraTrees trained on Original data and saved.\nGradientBoosting trained on Original data and saved.\nAdaBoost trained on Original data and saved.\nStacking trained on Original data and saved.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)","\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"/opt/conda/lib/python3.10/multiprocessing/pool.py\", line 51, in starmapstar\n    return list(itertools.starmap(args[0], args[1]))\n  File \"/tmp/ipykernel_552/3489688383.py\", line 41, in train_and_save\n    model.fit(X, y)\n  File \"/opt/conda/lib/python3.10/site-packages/sklearn/tree/_classes.py\", line 889, in fit\n    super().fit(\n  File \"/opt/conda/lib/python3.10/site-packages/sklearn/tree/_classes.py\", line 186, in fit\n    X, y = self._validate_data(\n  File \"/opt/conda/lib/python3.10/site-packages/sklearn/base.py\", line 579, in _validate_data\n    X = check_array(X, input_name=\"X\", **check_X_params)\n  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 921, in check_array\n    _assert_all_finite(\n  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 161, in _assert_all_finite\n    raise ValueError(msg_err)\nValueError: Input X contains NaN.\nDecisionTreeClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n\"\"\"","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[117], line 56\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m mp\u001b[38;5;241m.\u001b[39mPool(processes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmin\u001b[39m(mp\u001b[38;5;241m.\u001b[39mcpu_count(), \u001b[38;5;241m8\u001b[39m)) \u001b[38;5;28;01mas\u001b[39;00m pool:\n\u001b[0;32m---> 56\u001b[0m         \u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstarmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_and_save\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll models have been trained and saved on both original and resampled data.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/multiprocessing/pool.py:375\u001b[0m, in \u001b[0;36mPool.starmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstarmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    370\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;124;03m    Like `map()` method but the elements of the `iterable` are expected to\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;124;03m    be iterables as well and will be unpacked as arguments. Hence\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;124;03m    `func` and (a, b) becomes func(a, b).\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstarmapstar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/multiprocessing/pool.py:774\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n\u001b[1;32m    773\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 774\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n","\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nDecisionTreeClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"],"ename":"ValueError","evalue":"Input X contains NaN.\nDecisionTreeClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values","output_type":"error"}],"execution_count":117},{"cell_type":"code","source":"model_file_paths = [f\"{working_dir}{model_name}_{suffix}.pkl\" for model_name in models.keys() for suffix in [\"original\", \"resampled\"]]\n\n# Evaluate on the test data\nif __name__ == \"__main__\":\n    # Assuming X_test and y_test are already defined (from your test data)\n    # Load and evaluate each model\n    c=0\n    for model_file in model_file_paths:\n        c+=1\n        \n        load_and_evaluate(f\"{working_dir}{model_name}_{suffix}.pkl\", X_train, y_train, \"train\", \"original\")\n        load_and_evaluate(f\"{working_dir}{model_name}_{suffix}.pkl\", X_resampled, y_resampled, \"train\", \"Resampled\")\n\n        load_and_evaluate(model_file, X_val, y_val, \"validation\", \"Original\")\n        load_and_evaluate(model_file, X_val_resampled, y_val_resampled, \"validation\", \"Resampled\")\n        if c%2==0:\n            print()\n            with open(f\"{working_dir}validation.txt\", \"a\") as file:\n                file.write(\"\\n\")\n\n    print(\"Evaluation of all models completed.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T11:36:06.896799Z","iopub.status.idle":"2024-11-15T11:36:06.897238Z","shell.execute_reply.started":"2024-11-15T11:36:06.897027Z","shell.execute_reply":"2024-11-15T11:36:06.89705Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# predictions on test set\n\nfor model_file in model_file_paths:\n    with open(model_file, \"rb\") as file:\n        model = pickle.load(file)\n    \n    # Make predictions on the test data\n    y_pred = model.predict(X_test)\n\nwith open(f\"{working_dir}predictions.txt\", 'a') as file:\n    file.write(f\"{y_pred}\")\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T11:36:06.899085Z","iopub.status.idle":"2024-11-15T11:36:06.89953Z","shell.execute_reply.started":"2024-11-15T11:36:06.89931Z","shell.execute_reply":"2024-11-15T11:36:06.899334Z"}},"outputs":[],"execution_count":null}]}